version: '3'

services:
  postgres2:
    image: postgres:11-alpine
    restart: always
    environment: 
      - POSTGRES_USER=postgres_user
      - POSTGRES_PASSWORD=postgres_pass
      - POSTGRES_DB=postgres_db
      - POSTGRES_CONTAINER_NAME=postgres2
      - POSTGRES_PORT=13337
      - PGDATA=/var/lib/postgres/data
    ports: 
        - 13337:5432
    volumes:
        - ./data/postgres:/var/lib/postgresql/data
    networks: 
      - app-network

networks:
  app-network:
    driver: "bridge"


    
# Mini guide: 

# linux: If the docker daemon isn't set to start on system boot:
# > sudo dockerd

# windows: Make sure docker desktop is running.

# Check docker-compose version to verify that it is installed.
# > docker-compose --version

# Copy this file to a directory. Open a terminal and change the directory to the path of this file.
# > docker-compose up --build --remove-orphans

# Consider using -d to run it as a daemon:

# Shut down the docker containers with:
# > docker-compose down.

# Open dbeaver, pgadmin or whatever other client which supports postgresql.
# Check the screenshot and insert the same information. The password is postgres_pass
# (Check the contents of this file. It's the values under 'environment' (except POSTGRES_CONTAINER_NAME and PGDATA))


# == Tips ==:

# Containers can use a lot of space and ports and networks can possibly complicate the use of docker containers.
# Consider cleaning up at some point:
# docker image prune

# Or, clean up everything (networks, containers, images, etc):
# docker system prune 

# Make sure you use different ports for your containers. 
# Check the postgres container in this file, to see how you redirect from postgres' default port to, in this case, 13337.



# == More advanced tips ==


# Also something very important when/if we're going to use the databases in our projects.
# If you're using a backend container, consider connecting both the backend and the database containers to the same network
# E.g. (python backend container(The actual image is in the dockerfile)):

#  py_backend:
#    build:
#      context: ./backend
#      dockerfile: dockerfile
#    environment: 
#      - POSTGRES_USER=postgres_user
#      - POSTGRES_PASS=postgres_pass
#      - POSTGRES_DB=postgres_db
#      - POSTGRES_PORT=5432
#      - POSTGRES_HOST=postgres2
#    networks:
#      - app-network
#    ports:
#      - 8000:8000
#

# Whenever you do this, your connection string will look like this (python example( using psycopg2 driver from the sqlalchemy Object Relational Mapping framework)):

# conn_string = f'postgresql+psycopg2://{os.environ.get("POSTGRES_USER")}:{os.environ.get("POSTGRES_PASS")}@{os.environ.get("POSTGRES_HOST")}:{os.environ.get("POSTGRES_PORT")}/{os.environ.get("POSTGRES_DB")}'

# In other words, you will most likely not be using localhost like you do in your database client, but instead:
# postgres://user:pass@db_container_name:port/db_name
# Or (in this case): 
# postgres://postgres_user:postgres_pass@postgres2:13337/postgres_db

# Please note that this container is missing a dockerfile, which is specialized to the setup I'm using.
# As such, it won't work out of the box for you.



# As you might have figured out, the variables under 'environment: ' are actually environment variables.
# As you can probably also imagine, it's a pretty terrible idea having hard coded values in a file, describing the username and password of your database.
# I.e. it's a security hazard, especially when you start using public repositories.

# Consider grabbing the environment variables from your system instead. See the environment variables in this container (rust example(The actual image is in the dockerfile)):

#  hush.mutezone:
#    build: 
#      context: ./amb_back
#      dockerfile: Dockerfile
#    ports:
#      - "8000:8000"
#    container_name: amb_back
#    environment: 
#      - DATABASE_API_KEY=${DATABASE_API_KEY}
#      - POSTGRES_USER=${PG_USER}
#      - POSTGRES_PASSWORD=${PG_PASS}
#      - POSTGRES_DB=${PG_DB}
#      - AMBIENCE_JWT_TOKEN_SECRET=${AMBIENCE_JWT_TOKEN_SECRET}
#      - RUST_BACKTRACE=1
#      - RESOURCE_PATH=/usr/resources
#    volumes:
#      - ./amb_back/target:/usr/target
#      - ../resources:/usr/resources
#    networks:
#      - app-network     

# E.g. 'POSTGRES_USER=${PG_USER}' will use the environment variables on your system instead.

# On linux you can store the environment variables like this:

# In /etc/environment (or ~/.bash_profile or ~/.bashrc You'll figure it out):

# PG_USER=insert_some_database_username_here
# PG_PASS=insert_some_database_password_here
# PG_DB=insert_some_database_name_here

# Etc. This is also a setup you can use on your (cloud) server (assuming it runs on linux).

# On windows:
# 1. Open file explorer
# 2. Right click your pc
# 3. Properties
# 4. Advanced system settings
# 5. environment variables
# 6. Make entries for your environment variables

# It might be a better idea to use an nginx container for environment variables:
# https://hub.docker.com/_/nginx/
# You'll have to figure that one out by yourself as well.

# Also consider using secrets in your CI/CD setup (github actions, travis, jenkins). Then use something like:
# export={secret_transferred_via_secret, e.g. PG_USER}

# By doing this, your environment variables will be terminal specific.
# That's just an idea, however. Consider exploring it yourself.

# If you have any questions: cph-hw98@cphbusiness.dk

